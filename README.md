# Vision-Language Model (VLM) на основе CLIP

Этот проект представляет собой реализацию **Vision-Language Model (VLM)** с нуля, вдохновленную архитектурой CLIP (Contrastive Language–Image Pretraining). Модель обучается на датасете **Flickr30k**, который содержит изображения и их текстовые описания. В качестве энкодеров используются:

- **ResNet50** для кодирования изображений (ImageEncoder).
- **DistilBERT** для кодирования текста (TextEncoder).

Проект демонстрирует, как можно связать визуальную и текстовую информации для решения задач кросс-модального поиска и классификации.

---

## Описание проекта

Цель проекта — разработать модель, которая способна понимать взаимосвязь между изображениями и текстом. Модель обучается на основе контрастивного обучения (contrastive learning), где цель — максимизировать сходство между правильными парами "изображение-текст" и минимизировать сходство между неправильными.

### Основные компоненты:

1. **ImageEncoder**: Используется предобученная модель **ResNet50** для извлечения признаков из изображений.
2. **TextEncoder**: Используется предобученная модель **DistilBERT** для извлечения признаков из текстовых описаний.
3. **Контрастивная потеря**: Для обучения модели используется функция потерь, которая сравнивает эмбеддинги изображений и текстов.

---

    Более подробное описание решения находится в 'CLIP.ipynb'

---
